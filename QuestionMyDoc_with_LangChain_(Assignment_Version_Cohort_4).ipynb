{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEKghJQ2pmYH"
      },
      "source": [
        "### The Basics of LangChain\n",
        "\n",
        "In this notebook we'll explore exactly what LangChain is doing - and implement a straightforward example that lets us ask questions of a document!\n",
        "\n",
        "First things first, let's get our dependencies all set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXsYHTgvnCM2",
        "outputId": "1dfbdaf3-d881-42d0-e7e3-edd81fabd931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install openai langchain -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sLjfy8p3jf"
      },
      "source": [
        "You'll need to have an OpenAI API key for this next part - see [this](https://www.onmsft.com/how-to/how-to-get-an-openai-api-key/) if you haven't already set one up!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TTosnCHnGHG",
        "outputId": "29c15fea-3595-4569-8252-a0ec50d25198"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15M3Jx6SBXcO"
      },
      "source": [
        "#### Helper Functions (run this cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k3SBzWBUpQ21"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def disp_markdown(text: str) -> None:\n",
        "  display(Markdown(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU4LWrv-BayH"
      },
      "source": [
        "### Our First LangChain ChatModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-M-VQhQOC1c"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<div class=\"warn\">Note: Information on OpenAI's <a href=https://openai.com/pricing>pricing</a> and <a href=https://openai.com/policies/usage-policies>usage policies.</a></div>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVkfqk4NOFWS"
      },
      "source": [
        "Now that we're set-up with OpenAI's API - we can begin making our first ChatModel!\n",
        "\n",
        "There's a few important things to consider when we're using LangChain's ChatModel that are outlined [here](https://python.langchain.com/en/latest/modules/models/chat.html)\n",
        "\n",
        "Let's begin by initializing the model with OpenAI's `gpt-3.5-turbo` (ChatGPT) model.\n",
        "\n",
        "We're not going to be leveraging the [streaming](https://python.langchain.com/en/latest/modules/models/chat/examples/streaming.html) capabilities in this Notebook - just the basics to get us started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNscLft_nxBb",
        "outputId": "acc6cc50-2aab-491d-af08-b334dde03b72"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzGhlpwUPyU9"
      },
      "source": [
        "If we look at the [Chat completions](https://platform.openai.com/docs/guides/chat) documentation for OpenAI's chat models - we'll see that there are a few specific fields we'll need to concern ourselves with:\n",
        "\n",
        "`role`\n",
        "- This refers to one of three \"roles\" that interact with the model in specific ways.\n",
        "- The `system` role is an optional role that can be used to guide the model toward a specific task. Examples of `system` messages might be:\n",
        "  - You are an expert in Python, please answer questions as though we were in a peer coding session.\n",
        "  - You are the world's leading expert in stamps.\n",
        "\n",
        "  These messages help us \"prime\" the model to be more aligned with our desired task!\n",
        "\n",
        "- The `user` role represents, well, the user!\n",
        "- The `assistant` role lets us act in the place of the model's outputs. We can (and will) leverage this for some few-shot prompt engineering!\n",
        "\n",
        "Each of these roles has a class in LangChain to make it nice and easy for us to use!\n",
        "\n",
        "Let's look at an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dM7lciZtoPEp"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# The SystemMessage is associated with the system role, setting the scene for an Astronomy context\n",
        "system_message = SystemMessage(content=\"You are an astronomer at a space observatory.\")\n",
        "\n",
        "# The HumanMessage is associated with the user role, asking a question related to Astronomy\n",
        "user_message = HumanMessage(content=\"Can you explain the significance of the Hubble Deep Field?\")\n",
        "\n",
        "# The AIMessage is associated with the assistant role, providing an informative response\n",
        "assistant_message = AIMessage(content=\"Absolutely! The Hubble Deep Field is a groundbreaking image by the Hubble Space Telescope. It covers a small region in the constellation Ursa Major, depicting some of the youngest and most distant galaxies ever observed. This image has provided invaluable insights into the early universe, helping astronomers to understand galaxy formation and evolution.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSx5HBgjSUvB"
      },
      "source": [
        "Now that we have those messages set-up, let's send them to `gpt-3.5-turbo` with a new user message and see how it does!\n",
        "\n",
        "It's easy enough to do this - the ChatOpenAI model accepts a list of inputs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwDLOYOKSTpG",
        "outputId": "0ed9da50-1537-4e4e-cfc2-439e4f337638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The Large Synoptic Survey Telescope (LSST) is an upcoming ground-based telescope that will conduct a wide, fast, and deep survey of the entire southern sky. It will observe the night sky repeatedly over a ten-year period, creating a detailed map of the universe. The LSST is expected to revolutionize many areas of astronomy, including the study of dark matter and dark energy, the detection of asteroids, and the exploration of transient events such as supernovae. Its data will be made publicly available, allowing astronomers worldwide to access and study this wealth of information.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "second_user_message = HumanMessage(content=\"What about the LSST ?\")\n",
        "\n",
        "# create the list of prompts\n",
        "list_of_prompts = [\n",
        "    system_message,\n",
        "    user_message,\n",
        "    assistant_message,\n",
        "    second_user_message\n",
        "]\n",
        "\n",
        "# we can just call our chat_model on the list of prompts!\n",
        "chat_model.invoke(list_of_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZMYJDWXTkMq"
      },
      "source": [
        "Great! That's inline with what we expected to see!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DUNhabQUB8f"
      },
      "source": [
        "### PromptTemplates\n",
        "\n",
        "Next stop, we'll discuss a few templates. This allows us to easily interact with our model by not having to redo work we've already completed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "74vpojywT0-4"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "# we can signify variables we want access to by wrapping them in {}\n",
        "system_prompt_template = \"You are an expert in {SUBJECT}, and you're currently feeling {MOOD}\"\n",
        "system_prompt_template = SystemMessagePromptTemplate.from_template(system_prompt_template)\n",
        "\n",
        "user_prompt_template = \"{CONTENT}\"\n",
        "user_prompt_template = HumanMessagePromptTemplate.from_template(user_prompt_template)\n",
        "\n",
        "# put them together into a ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_prompt_template, user_prompt_template])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-nbEW-kV_na"
      },
      "source": [
        "Now that we have our `chat_prompt` set-up with the templates - let's see how we can easily format them with our content!\n",
        "\n",
        "NOTE: `disp_markdown` is just a helper function to display the formatted markdown response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "P4vd-W2FV7Xq",
        "outputId": "38de2425-5d00-4c59-c955-568ce3f0a8a7"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello! There are so many fascinating celestial objects to observe in the night sky, but some of the most popular and awe-inspiring ones include:\n",
              "\n",
              "1. **The Moon**: Our closest celestial neighbor, the Moon offers a wealth of detail to observe, from its craters and seas to its changing phases.\n",
              "\n",
              "2. **Planets**: Planets like Jupiter and Saturn are always popular targets for observation. Jupiter's cloud bands and four largest moons (Io, Europa, Ganymede, and Callisto) are especially fascinating, as are Saturn's iconic rings.\n",
              "\n",
              "3. **Nebulae**: Nebulae are vast clouds of gas and dust where stars are born. The Orion Nebula (M42) is a popular target, known for its colorful gases and young stars.\n",
              "\n",
              "4. **Galaxies**: The Andromeda Galaxy (M31) is a spectacular sight and the closest spiral galaxy to our own Milky Way. It's visible to the naked eye from dark skies and even more impressive through a telescope.\n",
              "\n",
              "5. **Star Clusters**: Open clusters like the Pleiades (M45) and globular clusters like M13 in Hercules are beautiful groupings of stars that are great for observing with binoculars or a telescope.\n",
              "\n",
              "6. **Meteor Showers**: While not individual objects, meteor showers can be incredibly exciting to observe. Events like the Perseids and Geminids can produce dozens of shooting stars per hour under dark skies.\n",
              "\n",
              "7. **Comets**: Occasionally, a bright comet will grace the night sky, offering a stunning and rare sight. Comets like Hale-Bopp and NEOWISE have been memorable in recent years.\n",
              "\n",
              "Each of these celestial objects offers a unique and captivating view of the universe, making stargazing a truly rewarding experience."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# note the method `to_messages()`, that's what converts our formatted prompt into\n",
        "formatted_chat_prompt = chat_prompt.format_prompt(SUBJECT=\"celestial objects\", MOOD=\"curiously excited\", CONTENT=\"Hi, what are the most fascinating celestial objects to observe in the night sky?\").to_messages()\n",
        "\n",
        "disp_markdown(chat_model.invoke(formatted_chat_prompt).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHehNFjAXbU_"
      },
      "source": [
        "### Putting the Chain in LangChain\n",
        "\n",
        "In essense, a chain is exactly as it sounds - it helps us chain actions together.\n",
        "\n",
        "Let's take a look at an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "lTzw4ZMoWX0X",
        "outputId": "e22f8122-96eb-410f-8117-8e2758e1622b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, the Andromeda Galaxy (M31) and the Milky Way are indeed on a collision course. Current scientific understanding suggests that the two galaxies are approaching each other at a speed of about 110 kilometers per second and are expected to collide in about 4.5 billion years. This collision will result in the formation of a new galaxy, often referred to as Milkomeda or Milkdromeda. The collision will be a spectacular event in cosmic terms and will likely reshape both galaxies as they merge and interact gravitationally. It's a truly awe-inspiring and humbling aspect of the vastness and dynamics of the universe."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
        "\n",
        "disp_markdown(chain.run(SUBJECT=\"galaxies\", MOOD=\"in awe\", CONTENT=\"Is the Andromeda Galaxy on a collision course with the Milky Way?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md5XYaAj_t51"
      },
      "source": [
        "### Index Local Files\n",
        "\n",
        "Now that we've got our first chain running, let's talk about indexing and what we can do with it!\n",
        "\n",
        "For the purposes of this tutorial, we'll be using the word \"index\" to refer to a collection of documents organized in a way that is easy for LangChain to access them as a \"Retriever\".\n",
        "\n",
        "Let's check out the Retriever set-up! First, a new dependency!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mkmPs3GAQMp",
        "outputId": "5d4cfb8d-317b-4923-cf2c-18eeddd33b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb tiktoken nltk -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dNp7hVQGLFn",
        "outputId": "ae656f34-20ab-4f32-a37e-726c3468c951"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/julien/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFe3eeJTB37W"
      },
      "source": [
        "Before we can get started with our chain - we'll have to include some kind of text that we want to include as potential context.\n",
        "\n",
        "Let's use Douglas Adam's [The Hitch Hiker's Guide to the Galaxy](https://erki.lap.ee/failid/raamatud/guide1.txt) as our text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vWU68TL2Acpe",
        "outputId": "bcfa7dce-d7fe-4a46-b840-1570dcde85b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/julien/code/JulsdL/huggingface_nlp'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdkQUoQICVwa",
        "outputId": "1ce41b02-8a74-4102-fd30-d94f9140b269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-20 02:32:41--  https://erki.lap.ee/failid/raamatud/guide1.txt\n",
            "Resolving erki.lap.ee (erki.lap.ee)... 185.158.177.102\n",
            "Connecting to erki.lap.ee (erki.lap.ee)|185.158.177.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 291862 (285K) [text/plain]\n",
            "Saving to: ‘guide1.txt’\n",
            "\n",
            "guide1.txt          100%[===================>] 285.02K  1.09MB/s    in 0.3s    \n",
            "\n",
            "2024-03-20 02:32:42 (1.09 MB/s) - ‘guide1.txt’ saved [291862/291862]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://erki.lap.ee/failid/raamatud/guide1.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W7zuITDYCaXo"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('guide1.txt', encoding='utf8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xha9YA4wA1-b"
      },
      "source": [
        "Now we can set up our first Index!\n",
        "\n",
        "More detail can be found [here](https://python.langchain.com/en/latest/modules/indexes/getting_started.html) but we'll skip to a more functional implementation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuuKSPgTB0Uz",
        "outputId": "c8833e83-d6dc-420d-849a-ed772ff203da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/julien/.pyenv/versions/3.11.3/envs/huggingface/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nQNNB8NC-XP"
      },
      "source": [
        "Now that we have our Index set-up, we can query it straight away!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7vSgpomC9n9",
        "outputId": "d609e246-f894-4951-cc38-aadb6e3160e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': \"What is the significance of the number 42 in 'The Hitchhiker's Guide to the Galaxy'?\",\n",
              " 'answer': \" The number 42 does not have any significance in 'The Hitchhiker's Guide to the Galaxy'.\\n\",\n",
              " 'sources': ''}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is the significance of the number 42 in 'The Hitchhiker's Guide to the Galaxy'?\"\n",
        "index.query_with_sources(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F5tv4ULERDo"
      },
      "source": [
        "### Putting it All Together\n",
        "\n",
        "Now that we have a simple idea of how we prompt, what a chain is, and has some local data - let's put it all together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uAHJHsksEx5H"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.indexes.vectorstore import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "O0mDrJVkAn7g"
      },
      "outputs": [],
      "source": [
        "with open(\"guide1.txt\") as f:\n",
        "    hitchhikersguide = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZDsNDVwBedJ"
      },
      "source": [
        "Next we'll want to split our text into appropirately sized chunks.\n",
        "\n",
        "We're going to be using the NLTKTextSplitter from LangChain today.\n",
        "\n",
        "The size of these chunks will depend heavily on a number of factors relating to which LLM you're using, what the max context size is, and more.\n",
        "\n",
        "You can also choose to have the chunks overlap to avoid potentially missing any important information between chunks. As we're dealing with a novel - there's not a critical need to include overlap.\n",
        "\n",
        "We can also pass in the separator - this is what we'll try and separate the documents on. Be careful to understand your documents so you can be sure you use a valid separator!\n",
        "\n",
        "For now, we'll go with 1000 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KP7poVVcAo_Q"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "text_splitter = NLTKTextSplitter()\n",
        "texts = text_splitter.split_text(hitchhikersguide)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEllUFEoBwlE"
      },
      "source": [
        "Now that we've split our document into more manageable sized chunks. We'll need to embed those documents!\n",
        "\n",
        "For more information on embedding - please check out this resource from OpenAI.\n",
        "\n",
        "In order to do this, we'll first need to select a method to embed - for this example we'll be using OpenAI's embedding - but you're free to use whatever you'd like.\n",
        "\n",
        "You just need to ensure you're using consistent embeddings as they don't play well with others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kIf3Vf81BvuI"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeVqHzANB_yG"
      },
      "source": [
        "\n",
        "Now that we've set up how we want to embed our document - we'll need to embed it.\n",
        "\n",
        "For this week we'll be glossing over the technical details of this process - as we'll get more into next week.\n",
        "\n",
        "Just know that we're converting our text into an easily queryable format!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4Z796M-aE_lU"
      },
      "outputs": [],
      "source": [
        "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcTYeo5ICSh7"
      },
      "source": [
        "Finally, we're able to combine what we've done so far into a chain!\n",
        "\n",
        "We're going to leverage the load_qa_chain to quickly integrate our queryable documents with an LLM.\n",
        "\n",
        "There are 4 major methods of building this chain, they can be found here!\n",
        "\n",
        "For this example we'll be using the stuff chain type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsEM-dR1FEvp",
        "outputId": "2e69abd9-632c-45e3-fc48-b0bffe636210"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'output_text': '\\n\\nThe maximum velocity of the space ship is not explicitly stated in the given context, but based on the description of the space ship being able to travel at R17 and above, it can be assumed that the maximum velocity is extremely high. However, it is also mentioned that the velocity can vary depending on the awareness of the third factor, and if not handled with tranquility, it can result in stress, ulcers, and even death. Additionally, in the given context, it is stated that the aircar rocketed them at speeds in excess of R17, indicating that the maximum velocity of the space ship could potentially be higher than R17. However, the exact maximum velocity of the space ship is still unknown and can vary depending on the circumstances.'}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\")\n",
        "query = \"What is the space ship maximum velocity ?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "chain.invoke({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBhx_8q20dq7"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM2ox3-eC04D"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
